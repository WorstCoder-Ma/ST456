{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-mPGVXOYeu"
      },
      "source": [
        "(LSE - ST456 DEEP LEARNING - WT2024)\n",
        "\n",
        "# XOR perceptron\n",
        "\n",
        "The XOR (or **exclusive OR**) problem is a classic problem in the field of artificial intelligence and machine learning. It is **a problem that cannot be solved by a single layer perceptron, and therefore requires a multi-layer perceptron** (from [here](https://www.educative.io/answers/xor-problem-in-neural-network)).\n",
        "\n",
        "This code is based on the Medium article: [How Neural Networks Solve the XOR Problem](https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7) and is intended to show that i) one single-layer perceptron is not capable of solving the XOR problem, and ii) how to combine multiple layers to circumvent this limitation and solve the problem.\n",
        "\n",
        "---\n",
        "\n",
        "There are placeholders along the code indicated by\n",
        "\n",
        "```\n",
        "## YOUR CODE HERE\n",
        "```\n",
        "that you will need to fill in. At the end of the notebook you will find an additional exercise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32MQcWN5spir"
      },
      "source": [
        "---\n",
        "\n",
        "## Python modules\n",
        "\n",
        "Apart from the usual visualization (`matplotlib` and `seaborn`) and numerical computation libraries (`numpy`), weâ€™ll use `cycle` from `itertools`. This is done since our algorithm cycles through our data indefinitely until it manages to correctly classify the entire training data without any mistakes in the middle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blWIZs1eUomj"
      },
      "source": [
        "# import necessary libraries\n",
        "from itertools import cycle\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWR8ZkDAy7Ey"
      },
      "source": [
        "## Plotting parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnLjNgmry7md"
      },
      "source": [
        "# helper parameters for plotting\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_context('poster')\n",
        "plt.rcParams[\"figure.figsize\"] = [8, 4]\n",
        "\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOYznPxVsxId"
      },
      "source": [
        "---\n",
        "\n",
        "## Training data\n",
        "\n",
        "We create our **training data**. This data is the same for each kind of logic gate, since they all take in **two boolean variables as input**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psNzlgg8SjOr"
      },
      "source": [
        "# training data for all types of logic gates\n",
        "train_data = np.array(\n",
        "    [\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]])\n",
        "\n",
        "# target solution for XOR\n",
        "target_xor = np.array(\n",
        "    [\n",
        "        [0],\n",
        "        [1],\n",
        "        [1],\n",
        "        [0]])\n",
        "\n",
        "# target solution for NAND\n",
        "target_nand = np.array(\n",
        "    [\n",
        "        [1],\n",
        "        [1],\n",
        "        [1],\n",
        "        [0]])\n",
        "\n",
        "# target solution for OR\n",
        "target_or = np.array(\n",
        "    [\n",
        "        [0],\n",
        "        [1],\n",
        "        [1],\n",
        "        [1]])\n",
        "\n",
        "# target solution for AND\n",
        "target_and = np.array(\n",
        "    [\n",
        "        [0],\n",
        "        [0],\n",
        "        [0],\n",
        "        [1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhvvxXDqs4_l"
      },
      "source": [
        "---\n",
        "\n",
        "## The Perceptron class\n",
        "\n",
        "To bring everything together, we create a simple **Perceptron class** with functions like `train()`, `forward()`, `classify()`, etc. We have some instance variables like the training data, the target, the number of input nodes and the learning rate.\n",
        "\n",
        "The class below has *docstrings* to explain what each function does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXK99PJGcdU5"
      },
      "source": [
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    Create a perceptron.\n",
        "    train_data: A 4x2 matrix with the input data.\n",
        "    target: A 4x1 matrix with the perceptron's expected outputs\n",
        "    lr: the learning rate. Defaults to 0.01\n",
        "    input_nodes: the number of nodes in the input layer of the perceptron.\n",
        "    It should be equal to the second dimension of train_data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_data, target, lr=0.01, input_nodes=2):\n",
        "        self.train_data = train_data\n",
        "        self.target = target\n",
        "        self.lr = lr\n",
        "        self.input_nodes = input_nodes\n",
        "\n",
        "        # randomly initialize the weights and set bias to -1.\n",
        "        self.w = np.random.uniform(size=self.input_nodes)\n",
        "        self.b = -1\n",
        "\n",
        "        # node_val holds the values of each node at a given point of time.\n",
        "        self.node_val = np.zeros(self.input_nodes)\n",
        "\n",
        "        # tracks how the number of consecutively correct classifications\n",
        "        # changes in each iteration\n",
        "        self.correct_iter = [0]\n",
        "\n",
        "    def _gradient(self, node, exp, output):\n",
        "        \"\"\"\n",
        "        Return the gradient for a weight.\n",
        "        This is the value of delta-w.\n",
        "        \"\"\"\n",
        "        return node * (exp - output)\n",
        "\n",
        "    def update_weights(self, exp, output):\n",
        "        \"\"\"\n",
        "        Update weights and bias based on their respective gradients\n",
        "        \"\"\"\n",
        "        for i in range(self.input_nodes):\n",
        "            self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n",
        "\n",
        "        # the value of the bias node can be considered as being 1 and the weight between this node\n",
        "        # and the output node being self.b\n",
        "        self.b += self.lr * self._gradient(1, exp, output)\n",
        "\n",
        "    def forward(self, datapoint):\n",
        "        \"\"\"\n",
        "        One forward pass through the perceptron.\n",
        "        Implementation of \"Xw + b\", where datapoint is X.\n",
        "        \"\"\"\n",
        "        return self.b + np.dot(self.w, datapoint)\n",
        "\n",
        "    def classify(self, datapoint):\n",
        "        \"\"\"\n",
        "        Return the class to which a datapoint belongs based on\n",
        "        the perceptron's output for that point.\n",
        "        \"\"\"\n",
        "        if self.forward(datapoint) >= 0:\n",
        "            # class 1\n",
        "            return 1\n",
        "        # otherwise, class 0\n",
        "        return 0\n",
        "\n",
        "    def plot(self, h=0.01):\n",
        "        \"\"\"\n",
        "        Generate plot of input data and decision boundary.\n",
        "        \"\"\"\n",
        "        # setting plot properties like size, theme and axis limits\n",
        "        sns.set_style('darkgrid')\n",
        "        plt.figure(figsize=(6, 6))\n",
        "\n",
        "        plt.axis('scaled')\n",
        "        plt.xlim(-0.1, 1.1)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "\n",
        "        colors = {\n",
        "            0: \"ro\",\n",
        "            1: \"go\"\n",
        "        }\n",
        "\n",
        "        for i in range(len(self.train_data)):\n",
        "            plt.plot([self.train_data[i][0]],\n",
        "                     [self.train_data[i][1]],\n",
        "                     colors[self.target[i][0]],\n",
        "                     markersize=20)\n",
        "\n",
        "        x_range = np.arange(-0.1, 1.1, h)\n",
        "        y_range = np.arange(-0.1, 1.1, h)\n",
        "\n",
        "        # creating a mesh to plot decision boundary\n",
        "        xx, yy = np.meshgrid(x_range, y_range, indexing='ij')\n",
        "        Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])\n",
        "\n",
        "        # using the contourf function to create the plot\n",
        "        plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train a single layer perceptron.\n",
        "        \"\"\"\n",
        "        # the number of consecutive correct classifications\n",
        "        correct_counter = 0\n",
        "        iterations = 0\n",
        "\n",
        "        for train, target in cycle(zip(self.train_data, self.target)):\n",
        "            # end if all points are correctly classified\n",
        "            if correct_counter == len(self.train_data):\n",
        "                break\n",
        "\n",
        "            # a single layer perceptron can't model the xor function\n",
        "            # so it'll never converge!\n",
        "            if iterations > 1000:\n",
        "                print(\"1000 iterations exceded without convergence!\\nA single layered perceptron can't handle the XOR problem.\")\n",
        "                break\n",
        "\n",
        "            output = self.classify(train)\n",
        "            self.node_val = train\n",
        "            iterations += 1\n",
        "\n",
        "            # if correctly classified, increment correct_counter\n",
        "            if output == target:\n",
        "                correct_counter += 1\n",
        "            else:\n",
        "                # if incorrectly classified, update weights and reset correct_counter\n",
        "                self.update_weights(target, output)\n",
        "                correct_counter = 0\n",
        "\n",
        "            self.correct_iter.append(correct_counter)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOhlPJR6unR3"
      },
      "source": [
        "---\n",
        "\n",
        "## XOR function - initial approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgY7fPV1usSK"
      },
      "source": [
        "A **single-layered perceptron isn't enough to model the 2D XOR function**, so training will never converge. Here, **convergence** means correctly classifying all training examples consecutively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrPCLrq1wE9Y"
      },
      "source": [
        "# instantiating and training a XOR perceptron\n",
        "p_xor = Perceptron(train_data, target_xor)\n",
        "p_xor.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA1Cgj_yuj_P"
      },
      "source": [
        "Remember that a perceptron must correctly classify the entire training data in one go. If we keep track of how many points it correctly classified consecutively, we get something like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwz2-qBNwPSK"
      },
      "source": [
        "plt.xlabel('Iteration steps')\n",
        "plt.ylabel('Correct datapoints')\n",
        "_ = plt.plot(p_xor.correct_iter[:150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZuPWRUwv0A"
      },
      "source": [
        "The algorithm only terminates when `correct_counter` hits 4 â€” which is the size of the training set â€” so this will go on indefinitely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7_8vu0RwxXn"
      },
      "source": [
        "---\n",
        "\n",
        "## OR function\n",
        "\n",
        "Weâ€™ll use the same Perceptron class as before, only that weâ€™ll train it on OR training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfMZhYbKAB9E"
      },
      "source": [
        "# instantiating and training an OR perceptron\n",
        "p_or = Perceptron(train_data, target_or)\n",
        "p_or.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwapmzeAw_jR"
      },
      "source": [
        "**This converges**, since the data for the OR function is linearly separable. If we plot the number of correctly classified consecutive datapoints as we did in our first attempt, we get the below plot. Itâ€™s clear that around iteration 50, it hits the value 4, meaning that it classified the entire dataset correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7UgZRM3AHHb"
      },
      "source": [
        "plt.xlabel('Iteration steps')\n",
        "plt.ylabel('Correct datapoints')\n",
        "_ = plt.plot(p_or.correct_iter[:150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktiqZ9xkxei8"
      },
      "source": [
        "The decision boundary plot looks like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCD9N5bOvUiR"
      },
      "source": [
        "p_or.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17n4BV1Hxjr0"
      },
      "source": [
        "---\n",
        "\n",
        "## NAND function\n",
        "\n",
        "Here, we model a NAND gate. Just like the OR part, weâ€™ll use the same code, but train the model on the NAND data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WwVrwXLIfVn"
      },
      "source": [
        "# instantiating and training a NAND perceptron\n",
        "p_nand = Perceptron(train_data, target_nand)\n",
        "p_nand.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THjMBnXDx6iw"
      },
      "source": [
        "After training, the following plots show that **our model converged on the NAND data** and mimics the NAND gate perfectly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N6EA0LIIfVs"
      },
      "source": [
        "plt.xlabel('Iteration steps')\n",
        "plt.ylabel('Correct datapoints')\n",
        "_ = plt.plot(p_nand.correct_iter[:600])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L_kZoDFyMPF"
      },
      "source": [
        "_ = p_nand.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7_UtzevyOcB"
      },
      "source": [
        "The plot above shows perfect convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0abr-kGb0Rpf"
      },
      "source": [
        "## Modelling the AND function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbml7FO2ypRt"
      },
      "source": [
        "Let's instantiate an AND gate and train it using the same approach that we used for the other gates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgxMmhyqL6Of"
      },
      "source": [
        "# instantiating and training an AND perceptron\n",
        "p_and = Perceptron(train_data, target_and)\n",
        "p_and.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "devcn3fPL6Om"
      },
      "source": [
        "plt.xlabel('Iteration steps')\n",
        "plt.ylabel('Correct datapoints')\n",
        "_ = plt.plot(p_and.correct_iter[:150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuphr7150Ldj"
      },
      "source": [
        "p_and.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSePvkOPyYAw"
      },
      "source": [
        "---\n",
        "\n",
        "## Modelling the XOR function by combining single-layered perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFSm0HCN0X1Z"
      },
      "source": [
        "### The XOR function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vh1fFdC0oB4"
      },
      "source": [
        "**The XOR function is a combination of an `AND`, `NAND` and `OR` gates**.\n",
        "\n",
        "In the article, we derived this result:\n",
        "```\n",
        "XOR(x1, x2) = AND(\n",
        "    OR(x1, x2),\n",
        "    NAND(x1, x2)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAeGr9zcUO_H"
      },
      "source": [
        "def XOR(x1, x2):\n",
        "    \"\"\"\n",
        "    Return the boolean XOR of x1 and x2 by combining\n",
        "    individual single-layered perceptrons\n",
        "    \"\"\"\n",
        "    # define the data points...\n",
        "    x = [x1, x2]\n",
        "    # ...and each type of perceptron\n",
        "    p_or = Perceptron(train_data, target_or)\n",
        "    p_nand = Perceptron(train_data, target_nand)\n",
        "    p_and = Perceptron(train_data, target_and)\n",
        "\n",
        "    # train each perceptron\n",
        "    p_or.train()\n",
        "    p_nand.train()\n",
        "    p_and.train()\n",
        "    # asks for the XOR result (classification)\n",
        "    return p_and.classify([p_or.classify(x), p_nand.classify(x)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEfLtare1jQ8"
      },
      "source": [
        "When we try and find the XOR of 1 and 1, we get 0 just like we expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlzFzR7OImT7"
      },
      "source": [
        "XOR(1, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iMCwXmvRRml"
      },
      "source": [
        "XOR(0, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4xzke7j0PEI"
      },
      "source": [
        "## Multilayer perceptron (MLP)\n",
        "\n",
        "A **multi-layered perceptron** can have hidden layers. There are no fixed rules on the number of hidden layers or the number of nodes in each layer of a network. The best performing models are obtained through **trial and error**.\n",
        "\n",
        "Letâ€™s go with a single hidden layer with two nodes in it. Weâ€™ll be using the **sigmoid function** in each hidden layer nodes and in the output node.\n",
        "\n",
        "---\n",
        "\n",
        "### The MLP class\n",
        "\n",
        "This class houses each component of our MLP, from training and the forward pass, to classifying and plotting the decision boundary. Check the docstrings below to get an idea of what each function or variable is for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7hTUbd_LzAy"
      },
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    Create a multi-layer perceptron.\n",
        "    train_data: A 4x2 matrix with the input data.\n",
        "    target: A 4x1 matrix with expected outputs\n",
        "    lr: the learning rate. Defaults to 0.1\n",
        "    num_epochs: the number of times the training data goes through the model\n",
        "        while training\n",
        "    num_input: the number of nodes in the input layer of the MLP.\n",
        "        Should be equal to the second dimension of train_data.\n",
        "    num_hidden: the number of nodes in the hidden layer of the MLP.\n",
        "    num_output: the number of nodes in the output layer of the MLP.\n",
        "        Should be equal to the second dimension of target.\n",
        "    \"\"\"\n",
        "    def __init__(self, train_data, target, lr=0.1, num_epochs=100, num_input=2, num_hidden=2, num_output=1):\n",
        "        self.train_data = train_data\n",
        "        self.target = target\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        # initialize both sets of weights and biases randomly\n",
        "        # - weights_01: weights between input and hidden layer\n",
        "        # - weights_12: weights between hidden and output layer\n",
        "        self.weights_01 = np.random.uniform(size=(num_input, num_hidden))\n",
        "        self.weights_12 = np.random.uniform(size=(num_hidden, num_output))\n",
        "\n",
        "        # - b01: biases for the  hidden layer\n",
        "        # - b12: bias for the output layer\n",
        "        self.b01 = np.random.uniform(size=(1,num_hidden))\n",
        "        self.b12 = np.random.uniform(size=(1,num_output))\n",
        "\n",
        "        self.losses = []\n",
        "\n",
        "    def update_weights(self):\n",
        "\n",
        "        # Calculate the squared error\n",
        "        loss = 0.5 * (self.target - self.output_final) ** 2\n",
        "        self.losses.append(np.sum(loss))\n",
        "\n",
        "        error_term = (self.target - self.output_final)\n",
        "\n",
        "        # the gradient for the hidden layer weights\n",
        "        grad01 = self.train_data.T @ (((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) * self._delsigmoid(self.hidden_out))\n",
        "\n",
        "        # the gradient for the output layer weights\n",
        "        grad12 = self.hidden_out.T @ (error_term * self._delsigmoid(self.output_final))\n",
        "\n",
        "        # updating the weights by the learning rate times their gradient\n",
        "        self.weights_01 += self.lr * grad01\n",
        "        self.weights_12 += self.lr * grad12\n",
        "\n",
        "        # update the biases the same way\n",
        "        self.b01 += np.sum(self.lr * ((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) * self._delsigmoid(self.hidden_out), axis=0)\n",
        "        self.b12 += np.sum(self.lr * error_term * self._delsigmoid(self.output_final), axis=0)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        The sigmoid activation function.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def _delsigmoid(self, x):\n",
        "        \"\"\"\n",
        "        The first derivative of the sigmoid function wrt x\n",
        "        \"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\"\n",
        "        A single forward pass through the network.\n",
        "        Implementation of Xw + b\n",
        "        \"\"\"\n",
        "        self.hidden_ = np.dot(batch, self.weights_01) + self.b01\n",
        "        self.hidden_out = self._sigmoid(self.hidden_)\n",
        "        self.output_ = np.dot(self.hidden_out, self.weights_12) + self.b12\n",
        "        self.output_final = self._sigmoid(self.output_)\n",
        "\n",
        "        return self.output_final\n",
        "\n",
        "    def classify(self, datapoint):\n",
        "        \"\"\"\n",
        "        Return the class to which a datapoint belongs based on\n",
        "        the perceptron's output for that point.\n",
        "        \"\"\"\n",
        "        datapoint = np.transpose(datapoint)\n",
        "        if self.forward(datapoint) >= 0.5:\n",
        "            return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def plot(self, h=0.01):\n",
        "        \"\"\"\n",
        "        Generate plot of input data and decision boundary.\n",
        "        \"\"\"\n",
        "        # setting plot properties like size, theme and axis limits\n",
        "        sns.set_style('darkgrid')\n",
        "        plt.figure(figsize=(6, 6))\n",
        "\n",
        "        plt.axis('scaled')\n",
        "        plt.xlim(-0.1, 1.1)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "\n",
        "        colors = {\n",
        "            0: \"ro\",\n",
        "            1: \"go\"\n",
        "        }\n",
        "\n",
        "        # plotting the four datapoints\n",
        "        for i in range(len(self.train_data)):\n",
        "            plt.plot([self.train_data[i][0]],\n",
        "                     [self.train_data[i][1]],\n",
        "                     colors[self.target[i][0]],\n",
        "                     markersize=20)\n",
        "\n",
        "        x_range = np.arange(-0.1, 1.1, h)\n",
        "        y_range = np.arange(-0.1, 1.1, h)\n",
        "\n",
        "        # creating a mesh to plot decision boundary\n",
        "        xx, yy = np.meshgrid(x_range, y_range, indexing='ij')\n",
        "        Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])\n",
        "\n",
        "        # using the contourf function to create the plot\n",
        "        plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train an MLP. Runs through the data num_epochs number of times.\n",
        "        A forward pass is done first, followed by a backward pass (backpropagation)\n",
        "        where the networks parameter's are updated.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.num_epochs):\n",
        "\n",
        "            self.forward(self.train_data)\n",
        "            self.update_weights()\n",
        "\n",
        "            if epoch % 500 == 0:\n",
        "                print(\"Loss: \", self.losses[epoch])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpGVGSpJ32rN"
      },
      "source": [
        "### Training the MLP\n",
        "\n",
        "Letâ€™s train our MLP with a learning rate of 0.2 over 8000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WoL50bVMJYd"
      },
      "source": [
        "# instantiating and training the MLP\n",
        "mlp = MLP(train_data, target_xor, 0.2, 8000)\n",
        "mlp.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ127PcK39ni"
      },
      "source": [
        "If we plot the values of our loss function, we get the following plot after about 8000 iterations, showing that **our model has indeed converged**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HKd4W4CMJVx"
      },
      "source": [
        "plt.xlabel('Iteration steps')\n",
        "plt.ylabel('Loss')\n",
        "_ = plt.plot(mlp.losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCUS7yAq4SEo"
      },
      "source": [
        "A clear **non-linear decision boundary** is created here with our generalized neural network (or MLP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ3fza9N3fmP"
      },
      "source": [
        "mlp.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "The implementation above of the MLP is done with a low-level API scheme: we had to code all the gradients of the loss function and the updates of the parameters. For the rest of the course, we will use the higher-level API offered by [Keras](https://keras.io/) to construct neural network architectures. In particular, we will use the [Sequential model](https://keras.io/guides/sequential_model/), which allows to construct multi-layer neural networks by stacking layers on top of each other.\n",
        "\n",
        "Exercise: **can you implement the MLP using the Sequential model?** You can follow the architecture we used before with a single hidden layer with two nodes in it and the sigmoid function in each hidden layer nodes and in the output node. For this exercise you will only need the Dense layer, the sigmoid activation function, the binary cross entropy loss function and the stochastic gradient descent as optimizer. After having defined the architecture, you will need to compile the model first (specify loss and optimizer) and then fit the model on the data. Many online tutorials and exercises are available to help you (Keras has a lot of documentation and examples), for example you can start with [this](https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/) or the [TensorFlow tutorials](https://www.tensorflow.org/tutorials)."
      ],
      "metadata": {
        "id": "tTUEz5mj0QuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "3-7yoBYHfFZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}