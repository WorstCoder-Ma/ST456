{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "oPR4ivrheD8_"
      },
      "source": [
        "(LSE - ST456 DEEP LEARNING - WT2024)\n",
        "\n",
        "# Logistic regression model\n",
        "\n",
        "This notebook implements a **logistic regression model from scratch** applied for a **binary classification** problem (Titanic example). It is intended to be used for self-study and as a suplement to lecture content.\n",
        "\n",
        "At the end, we show how to specify the same model using [TensorFlow Functional API](https://www.tensorflow.org/guide/keras/functional_api) (also known as *low-level API*)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Step 0: software setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "tensorflow"
        ],
        "id": "VmKEfc0GeD9B"
      },
      "outputs": [],
      "source": [
        "# importing necessary modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: data loading and exploration\n",
        "\n",
        "TensorFlow provides a version of the Titanic dataset; however, in this example we use a **modified version of the Titanic dataset** from [Kaggle](https://www.kaggle.com/azeembootwala/titanic). This version was adapted for logistic regression, and is available at the `data` folder. There are two files, namely:\n",
        "\n",
        "* **`train_data.csv`**: a dataset of 792 instances and 16 features. The `survived` column is the target variable.\n",
        "\n",
        "* **`testdata.csv`**: a dataset of 100 instances and 16 features. The `survived` column is the target variable.\n",
        "\n",
        "Arrangements for both files:\n",
        "\n",
        "* The `parch` and `sibsp` columns from the original data set were replaced by a single column called `Family size`.\n",
        "* All categorical data, like `Embarked` and `pclass` have been re-encoded using **one-hot encoding**.\n",
        "* Four more columns have been added, **re-engineered** from the `Name` column to `Title1` to `Title4` (Mr, Mrs, Master, Miss) signifying males and females depending on whether they were married or not. An additional analysis to see i) if `Married` people had more survival instincts or not, and ii) is the trend similar for both genders.\n",
        "* All **missing values** have been filled with a median of the column values.\n",
        "* All real valued data columns have been **normalised**."
      ],
      "metadata": {
        "id": "gt8LgjadxQLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 4,
        "tab": [
          "tensorflow"
        ],
        "id": "IJr8ZZAFeD9C"
      },
      "outputs": [],
      "source": [
        "# loading the training dataset\n",
        "# REMEMBER to upload this dataset into Colab\n",
        "df1 = pd.read_csv('/content/titanic_train_data.csv')\n",
        "# format (number of rows and columns)\n",
        "df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic information about the dataset (columns, data types, missing data)\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "TwwE3TuzTOju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **target** variable  is `Survived`; all other columns are **features**.\n",
        "\n",
        "A short description:\n",
        "\n",
        "* `Sex`: 0 or 1 => male or female\n",
        "* `Age`: value rescaled between 0 and 1\n",
        "* `Fare`: ticket price rescaled between 0 and 1\n",
        "* `Pclass_1` .. `Pclass_3`: Passenger class one-hot encoded\n",
        "* `Family_size`: value rescaled between 0 and 1.\n",
        "* `Title_1 .. Title_4`: mr, mrs, master, miss one-hot encoded\n",
        "* `Emb_1 .. Emb_3`: Embark location one-hot encoded.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "46khTaezTuTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example instances\n",
        "df1.sample(5)"
      ],
      "metadata": {
        "id": "we6TodnuTOoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Step 2: data pre-processing\n",
        "\n",
        "Let's first remove some unnecessary columns (not useful for the model) from the training dataset."
      ],
      "metadata": {
        "id": "LkaEsQyfjD2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing unnecessary columns\n",
        "# Unnamed and PassengerID won't influence our model\n",
        "df = df1.drop(['Unnamed: 0', 'PassengerId'], axis=1)"
      ],
      "metadata": {
        "id": "PHe3RAQLTOmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's **split the training dataset into features (`X`) and target (`Y`) variable**. We have a total of 792 examples. Therefore, the shape for `Y` is `(ùëö,1)` where `ùëö = 792`. For `X` we expect `(ùëö, 14)`, where the columns are the features."
      ],
      "metadata": {
        "id": "PoZxmemhD1Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the training dataset into features (X) and target (Y)\n",
        "X_train = df.iloc[:,1:].to_numpy()\n",
        "Y_train = df['Survived'].to_numpy()"
      ],
      "metadata": {
        "id": "aCI5njjNDt7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the structure\n",
        "X_train.shape, Y_train.shape"
      ],
      "metadata": {
        "id": "238WJ4CpDt-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transposing data - for compatibility\n",
        "X_train_T = np.transpose(X_train)\n",
        "Y_train_T = np.transpose(Y_train)\n",
        "# Checking the structure\n",
        "X_train_T.shape, Y_train_T.shape"
      ],
      "metadata": {
        "id": "zfHdOqa6wtUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the same with the **testing/validation dataset**."
      ],
      "metadata": {
        "id": "1d-tqjRqEb8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the testing dataset\n",
        "# REMEMBER to upload this dataset into Google Colab\n",
        "df2 = pd.read_csv('/content/titanic_test_data.csv')\n",
        "df2.shape"
      ],
      "metadata": {
        "id": "ouwFnD3zDu63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing unecessary features\n",
        "df2 = df2.drop(['Unnamed: 0', 'PassengerId'], axis=1)\n",
        "# splitting the testing features (X) and labels (Y)\n",
        "X_test = df2.iloc[:,1:].to_numpy()\n",
        "Y_test = df2['Survived'].to_numpy()\n",
        "# transposing data - for compatibility\n",
        "X_test_T = np.transpose(X_test)\n",
        "Y_test_T = np.transpose(Y_test)\n",
        "# Checking the structure\n",
        "X_test_T.shape, Y_test_T.shape"
      ],
      "metadata": {
        "id": "eE9dvG1TDu63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Step 3: regression model (from scratch)\n",
        "\n",
        "We will implement a logistic regression model from scratch, going through all the necessary steps (pipeline) for training and testing the model."
      ],
      "metadata": {
        "id": "VgGMQCtp9nS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1: model defintion\n",
        "\n",
        "Let's start by defining the **activation function**. We will use a custom `sigmoid` function for now."
      ],
      "metadata": {
        "id": "CVILEA1J_69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom sigmoid activation function\n",
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A"
      ],
      "metadata": {
        "id": "06gUeaxcIU3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is the **forward function**, which implements the dot product and makes use of the activation function.\n",
        "\n",
        "We can split these in two steps:\n",
        "\n",
        "$$Z = WX + b$$\n",
        "$$A = \\sigma(Z)$$\n"
      ],
      "metadata": {
        "id": "dPFO6vn-Xrcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom forward pass function\n",
        "def forward(X, W, b):\n",
        "    Z = np.dot(W.T, X) + b\n",
        "    A = sigmoid(Z)\n",
        "    return A"
      ],
      "metadata": {
        "id": "r15eqnc8Xrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **loss function** should be a **binary cross entropy**, as we have only two target classes (`survided = 1` or `0`).\n",
        "\n",
        "$$loss = -\\frac{1}{m}\\sum_{i=1}^{m} y\\log(A) + (1 - y)\\log(1 - A)$$\n"
      ],
      "metadata": {
        "id": "jkClkSJ2BDai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom loss function\n",
        "# epsilon is a small value we add to avoid log(0) calculation\n",
        "def loss(A, Y, epsilon = 1e-15):\n",
        "    m = len(A)\n",
        "    l = -1/m * np.sum(Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
        "    return l"
      ],
      "metadata": {
        "id": "F25yrmWTXrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is the **backwards pass**. For this, we would need to differentiate the loss function with `W` and `b`.\n",
        "\n",
        "$$\\frac{\\partial loss}{\\partial W} \\sum_{i=1}^{m} X(A - Y)\\top$$\n",
        "\n",
        "$$\\frac{\\partial loss}{\\partial b} \\sum_{i=1}^{m} (A - y)$$\n"
      ],
      "metadata": {
        "id": "T94Kjbc0D0ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom backward pass function\n",
        "def backward(X, Y, A):\n",
        "    m = len(yhat)\n",
        "    dW = 1/m * np.dot(X, (A - Y).T)\n",
        "    db = 1/m * np.sum(A - Y)\n",
        "    return (dW, db)"
      ],
      "metadata": {
        "id": "P-xWCJYbXrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step implements the **backpropagation function** for updating weights and bias."
      ],
      "metadata": {
        "id": "Sr2K08FTI3X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights and bias update\n",
        "def update(W, b, dW, db, learning_rate = 0.01):\n",
        "    W = W - learning_rate * dW\n",
        "    b = b - learning_rate * db\n",
        "    return (W, b)"
      ],
      "metadata": {
        "id": "HGtEGIRKXrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the activation function returns a probability between 0 and 1, we need a custom function to round values <= 0.5 to 0 and values > 0.5 to 1."
      ],
      "metadata": {
        "id": "eLAnLnKmIY9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom round function\n",
        "def roundValue(A):\n",
        "    return np.uint8(A > 0.5)"
      ],
      "metadata": {
        "id": "YHHpI6F_Xrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is the definition of our **accuracy metric**."
      ],
      "metadata": {
        "id": "koIdcFC3JEr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom accuracy function\n",
        "def accuracy(yhat, Y):\n",
        "    return round(np.sum(yhat==Y) / len(yhat) * 1000) / 10"
      ],
      "metadata": {
        "id": "PUSIpgRUIb_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2: model instantiation and training"
      ],
      "metadata": {
        "id": "GX4baICYJex_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialising model parameters\n",
        "\n",
        "# random seed (for reproducibility)\n",
        "np.random.seed(2024)\n",
        "\n",
        "# we have 14 features in the dataset\n",
        "W = 0.01 * np.random.randn(14)\n",
        "# and a constant bias\n",
        "b = 0\n",
        "\n",
        "# checking initial parameters\n",
        "print(\"Initial weights:\\n\", W, \"\\n\\nInitial bias: \", b)"
      ],
      "metadata": {
        "id": "CotggAnRUAOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training hyperparameters\n",
        "num_iterations = 200 # number of training steps (epochs)\n",
        "lr = 0.01            # learning rate"
      ],
      "metadata": {
        "id": "3DASbTM3Hd9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will record loss and accuracy for plotting\n",
        "losses, acces = [], []\n",
        "losses_test, acces_test = [], []\n",
        "# main training loop\n",
        "for i in range(num_iterations):\n",
        "    # forward pass\n",
        "    A = forward(X_train_T, W, b)\n",
        "    # loss calculation\n",
        "    l = loss(Y_train_T, A)\n",
        "    # round the predicted value\n",
        "    yhat = roundValue(A)\n",
        "    # accuracy calculation\n",
        "    acc = accuracy(yhat, Y_train_T)\n",
        "    # backpropagation pass - update weights and bias\n",
        "    dW, db = backward(X_train_T, Y_train_T, A)\n",
        "    W, b = update(W, b, dW, db, learning_rate=lr)\n",
        "    # keep record of loss and accurcy\n",
        "    losses.append(l)\n",
        "    acces.append(acc)\n",
        "\n",
        "    # Compute it on the test set\n",
        "    A_test = forward(X_test_T, W, b)\n",
        "    l_test = loss(Y_test_T, A_test)\n",
        "    yhat_test = roundValue(A_test)\n",
        "    acc_test = accuracy(yhat_test, Y_test_T)\n",
        "    losses_test.append(l_test)\n",
        "    acces_test.append(acc_test)\n",
        "    # checkpoint every 50 iterations\n",
        "    if i % 20 == 0:\n",
        "        print('Loss on train:', l, f'\\tAccuracy on train: {acc}%')\n",
        "        print('Loss on test:', l_test, f'\\tAccuracy on test: {acc_test}%')"
      ],
      "metadata": {
        "id": "dZXMYSYxXrcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3: visualising performance"
      ],
      "metadata": {
        "id": "DEC6mcacKXgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].plot(np.arange(len(losses)), losses, 'b-', label='train')\n",
        "ax[0].plot(np.arange(len(losses_test)), losses_test, color='orange', label='test')\n",
        "ax[0].legend()\n",
        "xlab, ylab = ax[0].set_xlabel('Epoch'), ax[0].set_ylabel('Loss')\n",
        "ax[1].plot(np.arange(len(acces)), acces, 'b-', label='train')\n",
        "ax[1].plot(np.arange(len(acces_test)), acces_test, color='orange', label='test')\n",
        "ax[1].legend()\n",
        "xlab, ylab = ax[1].set_xlabel('Epoch'), ax[1].set_ylabel('Accuracy')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "LVCylxMMWfnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 36,
        "id": "XDafuMUMa0VE"
      },
      "source": [
        "---\n",
        "\n",
        "## Implementation using the Functional API\n",
        "\n",
        "`[from documentation]:` The Keras functional API is a way to **create models that are more flexible** than the `keras.Sequential API` (covered in the seminars). The functional API can handle **models with non-linear topology, shared layers, and even multiple inputs or outputs**.\n",
        "\n",
        "The main idea is that **a deep learning model is usually a directed acyclic graph (DAG) of layers**. So the functional API is a way to build graphs of layers.\n",
        "\n",
        "The basic steps are:\n",
        "\n",
        "* specify the input layer (`tf.kerasInput`) with proper format\n",
        "* specify any intermediate (hidden) layers, such as `tf.keras.layers.Dense`, using the previous layer as input to the current layer\n",
        "* specify the output layer, with proper activation function\n",
        "* specify the model (`tf.keras.Model`) with proper inputs and outputs\n",
        "\n",
        "In the following example, we implement a single-layer linear regression model and use the same synthetic data for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input layer and input shape (14 in this case, referring to our features)\n",
        "input = tf.keras.Input(shape=(14,))\n",
        "# the logistic regression model and proper activation function, applied to the input features\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(input)\n",
        "# the model architecture - input and output layers\n",
        "model2 = tf.keras.Model(inputs=input, outputs=output)\n",
        "# model parameters - optimiser, loss function, and performance metrics\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['acc'])\n",
        "# we train the model for 200 few epochs\n",
        "nepochs = 200\n",
        "history = model2.fit(X_train, Y_train, epochs=nepochs, verbose=2, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "id": "o_WcKOmgb2Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 200"
      ],
      "metadata": {
        "id": "LCAYM3zDr1K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].plot(np.arange(nepochs), history.history['loss'], label='train')\n",
        "ax[0].plot(np.arange(nepochs), history.history['val_loss'], color='orange', label='test')\n",
        "ax[0].legend()\n",
        "ax[0].title.set_text('loss')\n",
        "ax[1].plot(np.arange(nepochs), history.history['acc'], label='train')\n",
        "ax[1].plot(np.arange(nepochs), history.history['val_acc'], color='orange', label='test')\n",
        "ax[1].title.set_text('acc')"
      ],
      "metadata": {
        "id": "X8Jce5Qipcw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l01RS1QBTscS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}